{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering with BERT (SQuAD 1.1)\n",
    "\n",
    "This notebook performs: EDA, preprocessing, fine-tuning BERT, evaluation, and inference.\n",
    "It uses Hugging Face datasets and transformers, and helper utilities in `src/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, install dependencies in the environment\n",
    "# !pip install -r ../requirements.txt\n",
    "import os, sys\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "print('Project root:', PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5dec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import load_squad, question_type_distribution, answer_length_distribution, sample_qas\n",
    "from src.preprocess import prepare_train_features, prepare_validation_features\n",
    "from src.train import train\n",
    "from src.infer import QAInference\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "ds = load_squad('1.1')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecb8792",
   "metadata": {},
   "source": [
    "## EDA: Question types and answer length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50925f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "qd = question_type_distribution(ds['train'])\n",
    "qd_sorted = sorted(qd.items(), key=lambda kv: kv[1], reverse=True)[:15]\n",
    "import pandas as pd\n",
    "pd.DataFrame(qd_sorted, columns=['Question_First_Token', 'Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6620c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "al = answer_length_distribution(ds['train'])\n",
    "al_series = pd.Series(al).sort_index()\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.lineplot(x=al_series.index, y=al_series.values)\n",
    "plt.title('Answer length distribution (first answer words)')\n",
    "plt.xlabel('Answer length (words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56d5594",
   "metadata": {},
   "source": [
    "## Print answers for 5 context-question pairs (ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3267df62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (ctx, q, answers) in enumerate(sample_qas(ds['train'], n=5), start=1):\n",
    "    print(f'Example {i}')\n",
    "    print('Question:', q)\n",
    "    print('Answer(s):', answers)\n",
    "    print('Context snippet:', ctx[:200].replace('\\n', ' ') + '...')\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune BERT on a small subset (for quick demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = train(\n",
    "    model_name='bert-base-uncased',\n",
    "    output_dir=os.path.join(PROJECT_ROOT, 'question-answering-system-with-BERT', 'models', 'qa-bert'),\n",
    "    epochs=1,\n",
    "    batch_size=8,\n",
    "    learning_rate=3e-5,\n",
    "    squad_version='1.1',\n",
    "    train_samples=500,  # reduce for quick run\n",
    "    eval_samples=100,   # reduce for quick run\n",
    ")\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: Ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = QAInference(model_dir=os.path.join(PROJECT_ROOT, 'question-answering-system-with-BERT', 'models', 'qa-bert'))\n",
    "context = ('BERT stands for Bidirectional Encoder Representations from Transformers. ' +\n",
    "           'It is a transformer-based machine learning technique for NLP developed by Google.')\n",
    "question = 'What does BERT stand for?'\n",
    "qa.predict(context, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web App\n",
    "Run the Flask app from a terminal:\n",
    "\n",
    "```bash\n",
    "export QA_MODEL_DIR=question-answering-system-with-BERT/models/qa-bert\n",
    "python question-answering-system-with-BERT/webapp/app.py\n",
    "```\n",
    "\n",
    "Open http://127.0.0.1:5000 and try your context and question."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
